<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Policies &mdash; Mines Research Computing  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=359985c6" />

  
    <link rel="canonical" href="https://rc-docs.mines.edu/pages/policies.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Computing Options at Mines" href="computing_options.html" />
    <link rel="prev" title="Systems" href="systems.html" />
<!-- Google Tag Manager -->

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':

new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],

j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=

'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);

})(window,document,'script','dataLayer','GTM-MXJV3MB');</script>

<!-- End Google Tag Manager -->

</head>

<body class="wy-body-for-nav">
<!-- Google Tag Manager (noscript) -->

<noscript><iframe src=https://www.googletagmanager.com/ns.html?id=GTM-MXJV3MB

height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<!-- End Google Tag Manager (noscript) -->


  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/Mines-RC-03.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">General Information</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview of Research Computing at Mines</a></li>
<li class="toctree-l1"><a class="reference internal" href="systems.html">Systems</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Policies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hpc-storage-rates-and-best-practices">HPC &amp; Storage Rates and Best Practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hpc-rates-wendian">HPC Rates (Wendian)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#checking-hpc-usage">Checking HPC usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#checking-job-efficiency">Checking job efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="#storage-rates">Storage Rates</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-policy">Data Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpc-etiquette">HPC Etiquette</a></li>
<li class="toctree-l2"><a class="reference internal" href="#login-management-node">Login/Management Node</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scratch-vs-home-directory">Scratch vs Home Directory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#home-directory-policy">Home Directory Policy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#scratch-policy">Scratch Policy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#slurm">Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#walltime-policy">Walltime Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#increase-the-amount-of-parallelism">Increase the amount of parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="#incorporate-checkpointing">Incorporate checkpointing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#high-performance-computing-hpc-node-life-cycle">High-Performance Computing (HPC) Node Life Cycle</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#policy-statement">Policy Statement</a></li>
<li class="toctree-l3"><a class="reference internal" href="#policy-details">Policy Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="#review-and-revision">Review and Revision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="computing_options.html">Computing Options at Mines</a></li>
<li class="toctree-l1"><a class="reference internal" href="consultations.html">Research Computing Consultation</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_system.html">The Module System</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="workshops.html">Workshops</a></li>
<li class="toctree-l1"><a class="reference internal" href="publications.html">Publications using Mines HPC Systems</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user_guides/new_user_guide.html">Getting Started (New User Guide)</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/connecting_to_systems.html">Connecting to Mines&#64;HPC Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/python_environments.html">Using Anaconda for python environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/matlab.html">Running MATLAB on HPC Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/job_specific_examples.html">Job Specific Examples (Under Construction)</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/advanced_slurm_guide.html">Advanced Slurm Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/job_efficiency_xdmod.html">Knowing your job efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/Parallel_Scaling_Guide.html">Parallel Scaling Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/Parallel_Scaling_Guide.html#References">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guides/archived_guides.html">Archived Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Budget Guidance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="budget_guidance/research_computing_resource_guidance.html">Research Computing Guidance</a></li>
<li class="toctree-l1"><a class="reference internal" href="budget_guidance/guidance-case-study-fenics.html">Case Study: Researcher using Open Source Finite Element Code FEniCS for modeling reacting flows</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Mines Research Computing</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Policies</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/pages/policies.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="policies">
<h1>Policies<a class="headerlink" href="#policies" title="Link to this heading"></a></h1>
<section id="hpc-storage-rates-and-best-practices">
<h2>HPC &amp; Storage Rates and Best Practices<a class="headerlink" href="#hpc-storage-rates-and-best-practices" title="Link to this heading"></a></h2>
<section id="hpc-rates-wendian">
<h3>HPC Rates (Wendian)<a class="headerlink" href="#hpc-rates-wendian" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Node Type</strong></p></th>
<th class="head"><p><strong>Rate per hour [USD]</strong></p></th>
<th class="head"><p><strong>CPU core</strong></p></th>
<th class="head"><p><strong>Memory per CPU core [GB]</strong></p></th>
<th class="head"><p><strong>GPU</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CPU</p></td>
<td><p>$0.02</p></td>
<td><p>1</p></td>
<td><p>5 or 10*</p></td>
<td><p>NA</p></td>
</tr>
<tr class="row-odd"><td><p>GPU enabled</p></td>
<td><p>$0.12**</p></td>
<td><p>6</p></td>
<td><p>48</p></td>
<td><p>1 x NVIDIA V100</p></td>
</tr>
</tbody>
</table>
<p><em>Last updated: 3/31/2023</em></p>
<p>*There are two types of CPU nodes on Wendian: (1) a “low” memory node of 192 GB, and (2) a “high” memory node of 384 GB node. Jobs will be routed to each of these nodes depending on requested resources.</p>
<p>**For GPU jobs, the V100 node has 4 GPU cards. For each GPU card you request, you automatically must pay for 6 CPU cores and 48 GB memory, since this is ¼ of the available compute resources on the GPU node.</p>
</section>
<section id="checking-hpc-usage">
<h3>Checking HPC usage<a class="headerlink" href="#checking-hpc-usage" title="Link to this heading"></a></h3>
<p>If you would like to check your usage for a given month, we have provided some convenient commands on Wendian.</p>
<p>To check usage as a user, use the command <code class="docutils literal notranslate"><span class="pre">getUtilizationByUser</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$janedoe@wendian002:[~]: getUtilizationByUser 
janedoe -- Cluster/Account/User Utilization 2023-04-01T00:00:00 - 2023-04-12T11:59:59 (993600 secs)
&quot;Account&quot;,&quot;User&quot;,&quot;Amount&quot;,&quot;Used&quot;
&quot;hpcgroup&quot;,&quot;janedoe - Jane Doe&quot;,$1.23,0
</pre></div>
</div>
<p>To check usage as a PI for all your users, use the command <code class="docutils literal notranslate"><span class="pre">getUtilizationByPI</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pi@wendian002:[~]: getUtilizationByPI
pi -- Cluster/Account/User Utilization 2023-04-01T00:00:00 - 2023-04-12T11:59:59 (993600 secs)
&quot;Account&quot;|&quot;User&quot;|&quot;Amount&quot;
&quot;hpcgroup&quot;,&quot;janedoe - Jane Doe&quot;,$1.23,0
&quot;hpcgroup&quot;,&quot;johnsmith - John Smith&quot;,$1000.00,0
</pre></div>
</div>
</section>
<section id="checking-job-efficiency">
<h3>Checking job efficiency<a class="headerlink" href="#checking-job-efficiency" title="Link to this heading"></a></h3>
<p>After a given job is complete, we have a tool installed called <code class="docutils literal notranslate"><span class="pre">reportseff</span></code> that allows one to quickly check the percent utilization of CPU and memory requested. This tool can let you check jobs for a given jobID, as well as check in a given job directory that has slurm output files.</p>
<p>Please refer to the GitHub page for more information: <a class="reference external" href="https://github.com/troycomi/reportseff">https://github.com/troycomi/reportseff</a></p>
<p>If you find that you are poorly utilizing CPU or memory resources, feel free to reach out to the <a class="reference external" href="https://helpcenter.mines.edu">Mines Help Center</a> for an <a class="reference external" href="https://helpcenter.mines.edu/TDClient/1946/Portal/Requests/ServiceDet?ID=30287">HPC Consultation</a>. Job efficiency likely needs to be solved by a case by case basis, and we can serve you best this way.</p>
</section>
<section id="storage-rates">
<h3>Storage Rates<a class="headerlink" href="#storage-rates" title="Link to this heading"></a></h3>
<p>The current storage rate policy is below:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Storage</p></th>
<th class="head"><p>Rate [USD/TB]</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Orebits, redundant</p></td>
<td><p>$2.00</p></td>
</tr>
</tbody>
</table>
<p><em>Last updated: 03/31/2023</em></p>
</section>
</section>
<section id="data-policy">
<h2>Data Policy<a class="headerlink" href="#data-policy" title="Link to this heading"></a></h2>
<p>There are a few data solutions we offer at Mines for research data:</p>
<ul class="simple">
<li><p>Orebits - High capacity research storage <em>not</em> connected to HPC</p></li>
</ul>
<p>The following are only on Wendian &amp; Mio:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/scratch</span></code> - Active research data, subject to 180 day data purge</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/projects</span></code> - Scratch research data used within a research project that shared with multiple users, also subject to 180 day data purge. The PI must request a projects directory with the list of authorized users.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/sets</span></code> - Long term data storage available on HPC. The PI must request a sets directory with a list of authorized users.</p></li>
</ul>
<p>Note that all data on these directories have no redudancy, so please keep up with your own backups of active research data.</p>
<p>Your account privileges may be suspended if we detect any attempt to evade the data purge policies (i.e. scripting the touching of files to keep them current)</p>
<p>The table below breaks down the purge policy and associated costs of each the data solutions:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Purge Policy</p></th>
<th class="head"><p>Cost</p></th>
<th class="head"><p>Redunancy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Scratch (<code class="docutils literal notranslate"><span class="pre">/scratch</span></code>)</p></td>
<td><p>&gt;180 days</p></td>
<td><p>Free</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p>Projects (<code class="docutils literal notranslate"><span class="pre">/projects</span></code>)</p></td>
<td><p>&gt;180 days</p></td>
<td><p>Free</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p>Wendian Long-Term Storage (<code class="docutils literal notranslate"><span class="pre">/sets</span></code>)</p></td>
<td><p>None</p></td>
<td><p>Free</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p>Orebits</p></td>
<td><p>None</p></td>
<td><p>$2/TB/month</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>Your account privileges may be suspended if we detect any attempt to evade the data purge policies (i.e. scripting the touching of files to keep them current)</p>
</div></blockquote>
<p><em>Last updated: 11/09/2023</em></p>
</section>
<section id="hpc-etiquette">
<h2>HPC Etiquette<a class="headerlink" href="#hpc-etiquette" title="Link to this heading"></a></h2>
</section>
<section id="login-management-node">
<h2>Login/Management Node<a class="headerlink" href="#login-management-node" title="Link to this heading"></a></h2>
<p>When you login one of our HPC systems, you login what is known as the “management” node. This node allows one to login to HPC systems and interface with the job scheduler SLURM. Additionally, the management node can also be used to edit files, create environment and compile codes. However as a general rule, <strong>running simulation software on the management node is prohibited.</strong> On both HPC systems, a software called <code class="docutils literal notranslate"><span class="pre">arbiter</span></code> monitors system resources used on the login node. If you are using too many CPU resources, an automated email will be sent to you Mines E-Mail, warning you and throttling your CPU usage. Once a cooldown period ends, your CPU allotment will return to normal.</p>
</section>
<section id="scratch-vs-home-directory">
<h2>Scratch vs Home Directory<a class="headerlink" href="#scratch-vs-home-directory" title="Link to this heading"></a></h2>
</section>
<section id="home-directory-policy">
<h2>Home Directory Policy<a class="headerlink" href="#home-directory-policy" title="Link to this heading"></a></h2>
<p>Every user has 20GB of data allocated to their <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> directory. A common issue with filling this storage is conda environments. You can clean your conda packages by following <a class="reference external" href="https://wpfiles.mines.edu/wp-content/uploads/ciarc/docs/pages/user_guides/python_environments.html#cleaning-up-conda-packages">this</a> page.</p>
<section id="scratch-policy">
<h3>Scratch Policy<a class="headerlink" href="#scratch-policy" title="Link to this heading"></a></h3>
<p>Files on <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>)  is a short-term shared filesystem for storing data currently necessary for active research projects. Subject to purge on a six-month (180 day) cycle. There are no limits (within reason) to amount of data.</p>
</section>
</section>
<section id="slurm">
<h2>Slurm<a class="headerlink" href="#slurm" title="Link to this heading"></a></h2>
<section id="walltime-policy">
<h3>Walltime Policy<a class="headerlink" href="#walltime-policy" title="Link to this heading"></a></h3>
<p>The standard maximum walltime is six days (144 hours):</p>
<p><code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">–time=144:00:00.</span></code></p>
<p>This policy is strictly enforced by HPC&#64;Mines.  In the event that the computational problem you are tasked with solving seems to require a walltime that exceeds 144 hours, we strongly encourage that you find alternative approaches to simply extending walltime.  Below are two possible approaches.</p>
<section id="increase-the-amount-of-parallelism">
<h4>Increase the amount of parallelism<a class="headerlink" href="#increase-the-amount-of-parallelism" title="Link to this heading"></a></h4>
<p>By increasing the number of cores/nodes used in your job, you can often decrease the total wall time needed. If your code is only a single-core workload, feel free to reach out to us for a <a class="reference external" href="https://helpcenter.mines.edu/TDClient/1946/Portal/Requests/ServiceDet?ID=33487">HPC technical consultation</a> for other workflow options.</p>
</section>
<section id="incorporate-checkpointing">
<h4>Incorporate checkpointing<a class="headerlink" href="#incorporate-checkpointing" title="Link to this heading"></a></h4>
<p>Checkpointing is the process of periodically saving the state of a code’s program execution so that it can be resumed at a later time.  This is extremely helpful in mitigating the effects on your calculation in the event of an unexpected crash or error.  By saving output periodically, or at a certain recurring point, and being able to restart the calculation using the saved output, a catastrophic loss of an entire days-long compute effort could be avoided.  Using checkpointing to intentionally restart a calculation at a reasonably estimated point is a recommended approach to remain within the six-day maximum walltime.</p>
<p>For more focused computational assistance, with the above situations and other compute aspects of your research, the HPC&#64;Mines team is available and willing to provide personal, one-on-one assistance.  Please <a class="reference external" href="https://helpcenter.mines.edu/TDClient/1946/Portal/Requests/ServiceDet?ID=33487">submit a help request</a> to start the process.  We also suggest consulting with members of your group or other peers currently using similar codes or applications; they may provide expedited answers to your questions, based on their experience.</p>
</section>
</section>
</section>
<section id="high-performance-computing-hpc-node-life-cycle">
<h2>High-Performance Computing (HPC) Node Life Cycle<a class="headerlink" href="#high-performance-computing-hpc-node-life-cycle" title="Link to this heading"></a></h2>
<section id="policy-statement">
<h3>Policy Statement<a class="headerlink" href="#policy-statement" title="Link to this heading"></a></h3>
<p>This policy outlines the support and decommissioning process for High-Performance Computing (HPC) nodes within our organization. It establishes the duration of support under a service contract, outlines the repair procedures after the contract ends, and sets the retirement timeframe of seven years for HPC nodes.</p>
</section>
<section id="policy-details">
<h3>Policy Details<a class="headerlink" href="#policy-details" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Support under Service Contract:</p>
<ol class="arabic simple">
<li><p>HPC nodes will be covered under a service contract for a specified duration, which will be determined during the procurement process. The service contract will include technical support, maintenance, and repair services.</p></li>
<li><p>The service contract duration will be communicated to the relevant stakeholders and documented by the HPC administration team.</p></li>
</ol>
</li>
<li><p>Post-Service Contract Repairs:</p>
<ol class="arabic simple">
<li><p>After the service contract ends, the HPC administration team will continue to support HPC nodes for repairs if they meet the following criteria:</p>
<ol class="arabic simple">
<li><p>The issue is identified as an easy fix that can be resolved without significant time, effort, or expense.</p></li>
<li><p>The necessary parts for repair are readily available and can be obtained within a reasonable timeframe and cost.</p></li>
</ol>
</li>
<li><p>Repairs falling outside the criteria mentioned above may be considered on a case-by-case basis, subject to approval by the designated authority based on factors such as the node’s importance, overall HPC system requirements, and financial considerations.</p></li>
</ol>
</li>
<li><p>Retirement at Seven Years:</p>
<ol class="arabic simple">
<li><p>At the seven-year mark from the date of initial deployment or purchase, HPC nodes will be decommissioned as part of the standard retirement process.</p></li>
<li><p>The HPC administration team will coordinate the retirement process with the affected users.</p></li>
<li><p>Upon retirement, the node will be securely removed from the HPC cluster, and any remaining components will be properly disposed of or repurposed as per applicable guidelines.</p></li>
</ol>
</li>
<li><p>Exceptional Cases:</p>
<ol class="arabic simple">
<li><p>In exceptional cases where repairing a node beyond the service contract period may be necessary due to critical dependencies or unique circumstances, a deviation from this policy may be considered.</p></li>
<li><p>Exceptions to the retirement timeframe or repair criteria must be approved by the research computing manager, following a thorough evaluation and justification process.</p></li>
</ol>
</li>
</ol>
</section>
<section id="review-and-revision">
<h3>Review and Revision<a class="headerlink" href="#review-and-revision" title="Link to this heading"></a></h3>
<p>This policy shall be reviewed periodically to ensure its effectiveness and relevance. Revisions to the policy may be made as necessary, with approval from the HPC steering committee.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="systems.html" class="btn btn-neutral float-left" title="Systems" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="computing_options.html" class="btn btn-neutral float-right" title="Computing Options at Mines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<jinja2.runtime.BlockReference object at 0x7fc37b92e1d0>
  <footer role="contentinfo">
  <div class="column left">
   <h3><br><strong>Mines Research Computing</strong><hr></h3>

<big><strong><a href="https://rc.mines.edu"><u>Research Computing (RC)</u></a></strong><br>
          Colorado School of Mines<br>
          1500 Illinois St., Golden, CO 80401<br>
          303-273-3000 / 800-446-9488 </big>
  </div>
 
   <div class="column middle">
<h3><br><strong>Quick Links</strong><hr></h3>
<big>
  <ul> 
    <li><a href="https://outlook.office365.com/owa/calendar/ResearchComputing@mines0.onmicrosoft.com/bookings/"><u>Schedule a Meeting</u></a></li>
    <li><a href="https://helpcenter.mines.edu/TDClient/1946/Portal/Requests/TicketRequests/NewForm?ID=4GCQlvW5OYk_&RequestorType=Service"><u>Submit an RC ticket</u></a></li>
    <li><a href="https://helpcenter.mines.edu"><u>Mines Help Center</u></a></li>
  </ul>
</big>
<hr>
<big>
  <ul>  
    <li><a href="https://ask.cyberinfrastructure.org/"><u>Ask.Ci</u></a> (External)</li>
    <li><a href="https://its.mines.edu/"><u>Mines ITS</u></a></li>
    <li><a href="https://library.mines.edu/"><u>Mines Library</u></a></li>
</ul>
</big>




</div>
  <div class="column right">
<h3><br><strong>About RC</strong><hr></h3>
<p>ITS’ Research Computing (RC) group works to identify research needs across the university and aims to provide comprehensive and innovative services and infrastructure to further research and meet our vision set by the Mines@150 strategic plan. <br>
</p><hr>

<p></p>
</div>


<div>
  <div><p style="text-align: center"><big><a href="https://www.mines.edu" target="_blank" ">Mines Home</a> <strong>|</strong> <a href="https://www.mines.edu/accessibility" target="_blank" ">Accessibility</a> <strong>|</strong> <a href="https://www.mines.edu/privacy" target="_blank" ">Privacy</a> <strong>|</strong> © 2022-24 Colorado School of Mines</big></p></div></div></div>  
      </div>
    
  </footer>

  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
   


</body>
</html>